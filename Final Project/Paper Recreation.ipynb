{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thumbs up? Sentiment Classification using Machine Learning Techniques\n",
    "<ul>\n",
    "    <li>by Bo Pang, Lillian Lee and Shivakumar Vaithyanathan</li>\n",
    "    <li>recreated by <b>Jan Kristoffer Cheng</b> and <b>Johansson Tan</b></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook tries to recreate the results from the paper of Pang, Lee and Vaithyanathan regarding sentiment classification. They used movie reviews from IMDb as their corpus and classified the reviews as having either a positive or negative sentiment. In order to be able to do binary classification, they built different models using different features and machine learning techniques. The machine learning techniques that they used were Naive Bayes, Maximum Entropy, and SVM, but this project will only use Naive Bayes and SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Reading the corpus</h3>\n",
    "\n",
    "The corpus is readily available online. It contains different versions with each version having cleaner data. The results from the paper used version 0.9, but we used version 1.0. \n",
    "\n",
    "The zip file when extracted is split into two folders neg and pos, with each having 700 text files falling into the corresponding category. The class <i>FileReader</i> reads all the files given a path. This is also where the punctuations are separated from the word to easily distinguish them later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative: 700\n",
      "Positive: 700\n",
      "Total: 1400\n"
     ]
    }
   ],
   "source": [
    "from file_reader import FileReader\n",
    "\n",
    "negPath = 'mix20_rand700_tokens_cleaned/tokens/neg/'\n",
    "posPath = 'mix20_rand700_tokens_cleaned/tokens/pos/'\n",
    "\n",
    "fileReader = FileReader()\n",
    "\n",
    "negatives = fileReader.getTexts(negPath)\n",
    "positives = fileReader.getTexts(posPath)\n",
    "allTexts = negatives + positives\n",
    "\n",
    "print('Negative:', len(negatives))\n",
    "print('Positive:', len(positives))\n",
    "print('Total:', len(allTexts))\n",
    "\n",
    "N = len(negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Appending tags for negations</h3>\n",
    "\n",
    "The class <i>TextNegator</i> appends <b>--n</b> to words between a negation and punctuation. The output from this function is used for unigrams. As an example, consider the sentence: <b>I don't like the movie. I didn't enjoy at all.</b> The punctuations from this example will be split from the word as this is already done in <i>FileReader</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I don't like--n the--n movie--n . I didn't enjoy--n at--n all--n .\"]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from features import TextNegator\n",
    "\n",
    "texts = [\"I don't like the movie . I didn't enjoy at all .\"]\n",
    "textNegator = TextNegator()\n",
    "textNegator.getNegated(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of negations and punctuations used for <i>TextNegator</i> are inside the features file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both texts in negative and positive will be processed by <i>TextNegator</i> for latter use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "negatedNegatives = textNegator.getNegated(negatives)\n",
    "negatedPositives = textNegator.getNegated(positives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<h3>Preparing libraries</h3>\n",
    "\n",
    "Before anything else, different libraries such as numpy and sklearn should be imported. They will be utilized in building the models later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from k_fold import KFoldBatcher\n",
    "\n",
    "nFold = 3\n",
    "nPerFold = int(N/nFold)\n",
    "print(nPerFold)\n",
    "\n",
    "kfold = KFold(nFold)\n",
    "results = {'features':[], 'nFeatures': [],'nb': [], 'svm': []}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the paper, we used 3-fold cross validation with each fold having 233 texts from each category. The class <i>KFoldBatcher</i> splits the dimensions and classes into batches for cross validation. The variable <i>results</i> will hold the different outputs such as features used, average number of features, and average accuracies of both Naive Bayes and SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Unigrams frequency</h3>\n",
    "\n",
    "```python\n",
    "class UnigramFeature:\n",
    "    def __init__(self):\n",
    "        self.unigrams = []\n",
    "    def process(self, negatedTexts):\n",
    "    def get(self, negatedTexts, type='pres'):\n",
    "```\n",
    "\n",
    "<i>UnigramFeature.process(negatedTexts)</i> saves all the unigrams that appeared at least 4 times in the training data and stores it in <i>self.unigrams</i>.\n",
    "\n",
    "<i>UnigramFeature.get(negatedTexts, type='freq')</i> returns the unigrams' frequencies of the negated texts as a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: unigrams frequency\n",
      "Number of Features: 12660\n",
      "Naive Bayes Accuracy: 0.778254649499\n",
      "SVM Accuracy: 0.782546494993\n"
     ]
    }
   ],
   "source": [
    "from features import UnigramFeature\n",
    "\n",
    "nbAccuracy = 0\n",
    "svmAccuracy = 0\n",
    "nFeatures = 0\n",
    "i = 0\n",
    "for trainIndex, testIndex in kfold.split(negatives):\n",
    "    unigramFeature = UnigramFeature()\n",
    "    unigramFeature.process([negatedNegatives[index] for index in trainIndex] + [negatedPositives[index] for index in trainIndex])\n",
    "    nFeatures += len(unigramFeature.unigrams)\n",
    "    \n",
    "    featuresNegative = unigramFeature.get(negatedNegatives, type='freq')\n",
    "    featuresPositive = unigramFeature.get(negatedPositives, type='freq')\n",
    "    \n",
    "    kfoldBatcher = KFoldBatcher(nFold, featuresNegative, featuresPositive)\n",
    "    trainX = kfoldBatcher.getTrainX(i)\n",
    "    trainY = kfoldBatcher.getTrainY(i)\n",
    "    \n",
    "    testX = kfoldBatcher.getTestX(i)\n",
    "    testY = kfoldBatcher.getTestY(i)\n",
    "    \n",
    "    nb = BernoulliNB()\n",
    "    nb.fit(trainX, trainY)\n",
    "    nbAccuracy += accuracy_score(nb.predict(testX), testY)\n",
    "\n",
    "    svm = LinearSVC()\n",
    "    svm.fit(trainX, trainY)\n",
    "    svmAccuracy += accuracy_score(svm.predict(testX), testY)\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "nbAccuracy /= nFold\n",
    "svmAccuracy /= nFold\n",
    "nFeatures = int(nFeatures/nFold)\n",
    "results['features'].append('unigrams')\n",
    "results['nFeatures'].append(nFeatures)\n",
    "results['nb'].append(nbAccuracy)\n",
    "results['svm'].append(svmAccuracy)\n",
    "\n",
    "print('Features: unigrams frequency')\n",
    "print('Number of Features:', nFeatures)\n",
    "print('Naive Bayes Accuracy:', nbAccuracy)\n",
    "print('SVM Accuracy:', svmAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Unigrams presence</h3>\n",
    "\n",
    "In contrast before, the models here utilize the presence of the unigrams. Thus, each feature will either have a value of 0 or 1.\n",
    "\n",
    "<i>UnigramFeature.get(negatedTexts, type='freq')</i> returns the unigrams' presence of the negated texts as a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: unigrams presence\n",
      "Number of Features: 12660\n",
      "Naive Bayes Accuracy: 0.778254649499\n",
      "SVM Accuracy: 0.792560801144\n"
     ]
    }
   ],
   "source": [
    "unigramFeaturesNegative = []\n",
    "unigramFeaturesPositive = []\n",
    "\n",
    "nbAccuracy = 0\n",
    "svmAccuracy = 0\n",
    "nFeatures = 0\n",
    "i = 0\n",
    "for trainIndex, testIndex in kfold.split(negatives):\n",
    "    unigramFeature = UnigramFeature()\n",
    "    unigramFeature.process([negatedNegatives[index] for index in trainIndex] + [negatedPositives[index] for index in trainIndex])\n",
    "    nFeatures += len(unigramFeature.unigrams)\n",
    "    \n",
    "    featuresNegative = unigramFeature.get(negatedNegatives, type='pres')\n",
    "    featuresPositive = unigramFeature.get(negatedPositives, type='pres')\n",
    "    \n",
    "    unigramFeaturesNegative.append(featuresNegative)\n",
    "    unigramFeaturesPositive.append(featuresPositive)\n",
    "\n",
    "    kfoldBatcher = KFoldBatcher(nFold, featuresNegative, featuresPositive)\n",
    "    \n",
    "    trainX = kfoldBatcher.getTrainX(i)\n",
    "    trainY = kfoldBatcher.getTrainY(i)\n",
    "    \n",
    "    testX = kfoldBatcher.getTestX(i)\n",
    "    testY = kfoldBatcher.getTestY(i)\n",
    "    \n",
    "    nb = BernoulliNB()\n",
    "    nb.fit(trainX, trainY)\n",
    "    nbAccuracy += accuracy_score(nb.predict(testX), testY)\n",
    "\n",
    "    svm = LinearSVC()\n",
    "    svm.fit(trainX, trainY)\n",
    "    svmAccuracy += accuracy_score(svm.predict(testX), testY)\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "nbAccuracy /= nFold\n",
    "svmAccuracy /= nFold\n",
    "nFeatures = int(nFeatures/nFold)\n",
    "results['features'].append('unigrams')\n",
    "results['nFeatures'].append(nFeatures)\n",
    "results['nb'].append(nbAccuracy)\n",
    "results['svm'].append(svmAccuracy)\n",
    "\n",
    "print('Features: unigrams presence')\n",
    "print('Number of Features:', nFeatures)\n",
    "print('Naive Bayes Accuracy:', nbAccuracy)\n",
    "print('SVM Accuracy:', svmAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Bigrams</h3>\n",
    "\n",
    "```python\n",
    "class BigramFeature:\n",
    "    def __init__(self):\n",
    "        self.bigrams = []\n",
    "    def process(self, texts):\n",
    "    def get(self, texts):\n",
    "```\n",
    "\n",
    "<i>BigramFeature.process(texts)</i> saves the top 16165 bigrams that appeared at least 7 times in the training data and stores it in <i>self.bigrams</i>.\n",
    "\n",
    "<i>BigramFeature.get(texts)</i> returns the presence of the bigrams of the texts as a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: bigrams\n",
      "Number of Features: 16165\n",
      "Naive Bayes Accuracy: 0.748927038627\n",
      "SVM Accuracy: 0.752503576538\n"
     ]
    }
   ],
   "source": [
    "from features import BigramFeature\n",
    "\n",
    "bigramFeaturesNegative = []\n",
    "bigramFeaturesPositive = []\n",
    "\n",
    "nbAccuracy = 0\n",
    "svmAccuracy = 0\n",
    "nFeatures = 0\n",
    "i = 0\n",
    "for trainIndex, testIndex in kfold.split(negatives):\n",
    "    bigramFeature = BigramFeature()\n",
    "    bigramFeature.process([negatives[index] for index in trainIndex] + [positives[index] for index in trainIndex])\n",
    "    nFeatures += len(bigramFeature.bigrams)\n",
    "    \n",
    "    featuresNegative = bigramFeature.get(negatives)\n",
    "    featuresPositive = bigramFeature.get(positives)\n",
    "    \n",
    "    bigramFeaturesNegative.append(featuresNegative)\n",
    "    bigramFeaturesPositive.append(featuresPositive)\n",
    "\n",
    "    kfoldBatcher = KFoldBatcher(nFold, featuresNegative, featuresPositive)\n",
    "    \n",
    "    trainX = kfoldBatcher.getTrainX(i)\n",
    "    trainY = kfoldBatcher.getTrainY(i)\n",
    "    \n",
    "    testX = kfoldBatcher.getTestX(i)\n",
    "    testY = kfoldBatcher.getTestY(i)\n",
    "    \n",
    "    nb = BernoulliNB()\n",
    "    nb.fit(trainX, trainY)\n",
    "    nbAccuracy += accuracy_score(nb.predict(testX), testY)\n",
    "\n",
    "    svm = LinearSVC()\n",
    "    svm.fit(trainX, trainY)\n",
    "    svmAccuracy += accuracy_score(svm.predict(testX), testY)\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "nbAccuracy /= nFold\n",
    "svmAccuracy /= nFold\n",
    "nFeatures = int(nFeatures/nFold)\n",
    "results['features'].append('bigrams')\n",
    "results['nFeatures'].append(nFeatures)\n",
    "results['nb'].append(nbAccuracy)\n",
    "results['svm'].append(svmAccuracy)\n",
    "\n",
    "print('Features: bigrams')\n",
    "print('Number of Features:', nFeatures)\n",
    "print('Naive Bayes Accuracy:', nbAccuracy)\n",
    "print('SVM Accuracy:', svmAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Unigrams and Bigrams</h3>\n",
    "\n",
    "The models here utilize both unigrams and bigrams, which is essentially concatenating both features before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-1ea6c00dd071>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnFold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mfeaturesNegative\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munigramFeaturesNegative\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbigramFeaturesNegative\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mfeaturesPositive\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munigramFeaturesPositive\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbigramFeaturesPositive\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "nbAccuracy = 0\n",
    "svmAccuracy = 0\n",
    "nFeatures = 0\n",
    "for i in range(nFold):\n",
    "    print(i)\n",
    "    featuresNegative = np.concatenate((unigramFeaturesNegative[i], bigramFeaturesNegative[i]), axis=1)\n",
    "    featuresPositive = np.concatenate((unigramFeaturesPositive[i], bigramFeaturesPositive[i]), axis=1)\n",
    "\n",
    "    kfoldBatcher = KFoldBatcher(nFold, featuresNegative, featuresPositive)\n",
    "    \n",
    "    trainX = kfoldBatcher.getTrainX(i)\n",
    "    trainY = kfoldBatcher.getTrainY(i)\n",
    "    \n",
    "    testX = kfoldBatcher.getTestX(i)\n",
    "    testY = kfoldBatcher.getTestY(i)\n",
    "    \n",
    "    nb = BernoulliNB()\n",
    "    nb.fit(trainX, trainY)\n",
    "    nbAccuracy += accuracy_score(nb.predict(testX), testY)\n",
    "\n",
    "    svm = LinearSVC()\n",
    "    svm.fit(trainX, trainY)\n",
    "    svmAccuracy += accuracy_score(svm.predict(testX), testY)\n",
    "    \n",
    "nbAccuracy /= nFold\n",
    "svmAccuracy /= nFold\n",
    "nFeatures = int(nFeatures/nFold)\n",
    "results['features'].append('unigrams+bigrams')\n",
    "results['nFeatures'].append(nFeatures)\n",
    "results['nb'].append(nbAccuracy)\n",
    "results['svm'].append(svmAccuracy)\n",
    "\n",
    "print('Features: unigrams+bigrams presence')\n",
    "print('Number of Features:', nFeatures)\n",
    "print('Naive Bayes Accuracy:', nbAccuracy)\n",
    "print('SVM Accuracy:', svmAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Unigrams + POS</h3>\n",
    "\n",
    "The class <i>POSTagger</i> returns the part-of-speech sequence using the nltk library given a string. As an example, consider the sentence: <b>The movie was really great! I didn't expect that plot twist!</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['DT',\n",
       "  'NN',\n",
       "  'VBD',\n",
       "  'RB',\n",
       "  'JJ',\n",
       "  '.',\n",
       "  'PRP',\n",
       "  'VBP',\n",
       "  'VB',\n",
       "  'IN',\n",
       "  'NN',\n",
       "  'NN',\n",
       "  '.']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from features import POSTagger\n",
    "\n",
    "texts = [\"The movie was really great ! I didn't expect that plot twist !\"]\n",
    "posTagger = POSTagger()\n",
    "posTagger.getPOS(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the POS sequences of the corpus for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posNegatives = posTagger.getPOS(negatives)\n",
    "posPositives = posTagger.getPOS(positives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class UnigramPOSFeature:\n",
    "    def __init__(self):\n",
    "        self.unigrams = []\n",
    "    def process(self, negatedTexts, posOfTexts):\n",
    "    def get(self, negatedTexts, posOfTexts):\n",
    "```\n",
    "\n",
    "<i>UnigramPOSFeature.process(negatedTexts, posOfTexts)</i> saves the unique unigrams and POS combination that appeared at least 4 times in the training data and stores it in <i>self.unigrams</i>.\n",
    "\n",
    "<i>UnigramPOSFeature.get(negatedTexts, posOfTexts)</i> returns the presence of the unigrams of the texts as a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: unigrams+POS\n",
      "Number of Features: 13707\n",
      "Naive Bayes Accuracy: 0.776824034335\n",
      "SVM Accuracy: 0.79113018598\n"
     ]
    }
   ],
   "source": [
    "from features import UnigramPOSFeature\n",
    "\n",
    "nbAccuracy = 0\n",
    "svmAccuracy = 0\n",
    "nFeatures = 0\n",
    "i = 0\n",
    "for trainIndex, testIndex in kfold.split(negatives):\n",
    "    unigramPOSFeature = UnigramPOSFeature()\n",
    "    negatedTextsTrain = [negatedNegatives[index] for index in trainIndex] + [negatedPositives[index] for index in trainIndex]\n",
    "    posTextsTrain = [posNegatives[index] for index in trainIndex] + [posPositives[index] for index in trainIndex]\n",
    "    unigramPOSFeature.process(negatedTextsTrain, posTextsTrain)\n",
    "    nFeatures += len(unigramPOSFeature.unigrams)\n",
    "    \n",
    "    featuresNegative = unigramPOSFeature.get(negatedNegatives, posNegatives)\n",
    "    featuresPositive = unigramPOSFeature.get(negatedPositives, posPositives)\n",
    "\n",
    "    kfoldBatcher = KFoldBatcher(nFold, featuresNegative, featuresPositive)\n",
    "    \n",
    "    trainX = kfoldBatcher.getTrainX(i)\n",
    "    trainY = kfoldBatcher.getTrainY(i)\n",
    "    \n",
    "    testX = kfoldBatcher.getTestX(i)\n",
    "    testY = kfoldBatcher.getTestY(i)\n",
    "    \n",
    "    nb = BernoulliNB()\n",
    "    nb.fit(trainX, trainY)\n",
    "    nbAccuracy += accuracy_score(nb.predict(testX), testY)\n",
    "\n",
    "    svm = LinearSVC()\n",
    "    svm.fit(trainX, trainY)\n",
    "    svmAccuracy += accuracy_score(svm.predict(testX), testY)\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "nbAccuracy /= nFold\n",
    "svmAccuracy /= nFold\n",
    "nFeatures = int(nFeatures/nFold)\n",
    "results['features'].append('unigrams+POS')\n",
    "results['nFeatures'].append(nFeatures)\n",
    "results['nb'].append(nbAccuracy)\n",
    "results['svm'].append(svmAccuracy)\n",
    "\n",
    "print('Features: unigrams+POS')\n",
    "print('Number of Features:', nFeatures)\n",
    "print('Naive Bayes Accuracy:', nbAccuracy)\n",
    "print('SVM Accuracy:', svmAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from features import AdjectiveFeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: adjectives\n",
      "Number of Features: 11261\n",
      "Naive Bayes Accuracy: 0.777539341917\n",
      "SVM Accuracy: 0.76251788269\n"
     ]
    }
   ],
   "source": [
    "nbAccuracy = 0\n",
    "svmAccuracy = 0\n",
    "nFeatures = 0\n",
    "i = 0\n",
    "for trainIndex, testIndex in kfold.split(negatives):\n",
    "    adjFeature = AdjectiveFeature()\n",
    "    textsTrain = [negatives[index] for index in trainIndex] + [positives[index] for index in trainIndex]\n",
    "    posTextsTrain = [posNegatives[index] for index in trainIndex] + [posPositives[index] for index in trainIndex]\n",
    "    adjFeature.process(textsTrain, posTextsTrain)\n",
    "    nFeatures += len(adjFeature.adjectives)\n",
    "    \n",
    "    featuresNegative = adjFeature.get(negatives)\n",
    "    featuresPositive = adjFeature.get(positives)\n",
    "\n",
    "    kfoldBatcher = KFoldBatcher(nFold, featuresNegative, featuresPositive)\n",
    "    \n",
    "    trainX = kfoldBatcher.getTrainX(i)\n",
    "    trainY = kfoldBatcher.getTrainY(i)\n",
    "    \n",
    "    testX = kfoldBatcher.getTestX(i)\n",
    "    testY = kfoldBatcher.getTestY(i)\n",
    "    \n",
    "    nb = BernoulliNB()\n",
    "    nb.fit(trainX, trainY)\n",
    "    nbAccuracy += accuracy_score(nb.predict(testX), testY)\n",
    "\n",
    "    svm = LinearSVC()\n",
    "    svm.fit(trainX, trainY)\n",
    "    svmAccuracy += accuracy_score(svm.predict(testX), testY)\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "nbAccuracy /= nFold\n",
    "svmAccuracy /= nFold\n",
    "nFeatures = int(nFeatures/nFold)\n",
    "results['features'].append('adjectives')\n",
    "results['nFeatures'].append(nFeatures)\n",
    "results['nb'].append(nbAccuracy)\n",
    "results['svm'].append(svmAccuracy)\n",
    "\n",
    "print('Features: adjectives')\n",
    "print('Number of Features:', nFeatures)\n",
    "print('Naive Bayes Accuracy:', nbAccuracy)\n",
    "print('SVM Accuracy:', svmAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 2633 Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: top 2633 unigrams\n",
      "Number of Features: 2633\n",
      "Naive Bayes Accuracy: 0.778254649499\n",
      "SVM Accuracy: 0.792560801144\n"
     ]
    }
   ],
   "source": [
    "nbAccuracy = 0\n",
    "svmAccuracy = 0\n",
    "nFeatures = 0\n",
    "for i in range(nFold):\n",
    "    featuresNegative = unigramFeaturesNegative[i][:2633]\n",
    "    featuresPositive = unigramFeaturesPositive[i][:2633]\n",
    "\n",
    "    kfoldBatcher = KFoldBatcher(nFold, featuresNegative, featuresPositive)\n",
    "    \n",
    "    trainX = kfoldBatcher.getTrainX(i)\n",
    "    trainY = kfoldBatcher.getTrainY(i)\n",
    "    \n",
    "    testX = kfoldBatcher.getTestX(i)\n",
    "    testY = kfoldBatcher.getTestY(i)\n",
    "    \n",
    "    nb = BernoulliNB()\n",
    "    nb.fit(trainX, trainY)\n",
    "    nbAccuracy += accuracy_score(nb.predict(testX), testY)\n",
    "\n",
    "    svm = LinearSVC()\n",
    "    svm.fit(trainX, trainY)\n",
    "    svmAccuracy += accuracy_score(svm.predict(testX), testY)\n",
    "    \n",
    "nbAccuracy /= nFold\n",
    "svmAccuracy /= nFold\n",
    "nFeatures = 2633\n",
    "results['features'].append('top 2633 unigrams')\n",
    "results['nFeatures'].append(nFeatures)\n",
    "results['nb'].append(nbAccuracy)\n",
    "results['svm'].append(svmAccuracy)\n",
    "\n",
    "print('Features: top 2633 unigrams')\n",
    "print('Number of Features:', nFeatures)\n",
    "print('Naive Bayes Accuracy:', nbAccuracy)\n",
    "print('SVM Accuracy:', svmAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigrams + position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from features import PositionTagger\n",
    "\n",
    "positionTagger = PositionTagger()\n",
    "positionNegatives = positionTagger.getPositions(negatedNegatives)\n",
    "positionPositives = positionTagger.getPositions(negatedPositives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from features import UnigramPositionFeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: unigrams+position\n",
      "Number of Features: 16842\n",
      "Naive Bayes Accuracy: 0.774678111588\n",
      "SVM Accuracy: 0.786123032904\n"
     ]
    }
   ],
   "source": [
    "nbAccuracy = 0\n",
    "svmAccuracy = 0\n",
    "nFeatures = 0\n",
    "i = 0\n",
    "for trainIndex, testIndex in kfold.split(negatives):\n",
    "    unigramPositionFeature = UnigramPositionFeature()\n",
    "    negatedTextsTrain = [negatedNegatives[index] for index in trainIndex] + [negatedPositives[index] for index in trainIndex]\n",
    "    positionTextsTrain = [positionNegatives[index] for index in trainIndex] + [positionPositives[index] for index in trainIndex]\n",
    "    unigramPositionFeature.process(negatedTextsTrain, positionTextsTrain)\n",
    "    nFeatures += len(unigramPositionFeature.unigrams)\n",
    "    \n",
    "    featuresNegative = unigramPositionFeature.get(negatedNegatives, positionNegatives)\n",
    "    featuresPositive = unigramPositionFeature.get(negatedPositives, positionPositives)\n",
    "\n",
    "    kfoldBatcher = KFoldBatcher(nFold, featuresNegative, featuresPositive)\n",
    "    \n",
    "    trainX = kfoldBatcher.getTrainX(i)\n",
    "    trainY = kfoldBatcher.getTrainY(i)\n",
    "    \n",
    "    testX = kfoldBatcher.getTestX(i)\n",
    "    testY = kfoldBatcher.getTestY(i)\n",
    "    \n",
    "    nb = BernoulliNB()\n",
    "    nb.fit(trainX, trainY)\n",
    "    nbAccuracy += accuracy_score(nb.predict(testX), testY)\n",
    "\n",
    "    svm = LinearSVC()\n",
    "    svm.fit(trainX, trainY)\n",
    "    svmAccuracy += accuracy_score(svm.predict(testX), testY)\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "nbAccuracy /= nFold\n",
    "svmAccuracy /= nFold\n",
    "nFeatures = int(nFeatures/nFold)\n",
    "results['features'].append('unigrams+bigrams')\n",
    "results['nFeatures'].append(nFeatures)\n",
    "results['nb'].append(nbAccuracy)\n",
    "results['svm'].append(svmAccuracy)\n",
    "\n",
    "print('Features: unigrams+position')\n",
    "print('Number of Features:', nFeatures)\n",
    "print('Naive Bayes Accuracy:', nbAccuracy)\n",
    "print('SVM Accuracy:', svmAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unigramPositionFeature.unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(negatives[0])\n",
    "negates = textNegator.getNegated(negatives[:2])\n",
    "print(negates[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'features': ['unigrams', 'unigrams', 'bigrams', 'bigrams', 'bigrams', 'adjectives', 'unigrams+POS', 'top 2633 unigrams', 'unigrams+bigrams'], 'svm': [0.78254649499284701, 0.79256080114449212, 0.5, 0.5, 0.75250357653791122, 0.76251788268955645, 0.79113018597997131, 0.79256080114449212, 0.78612303290414876], 'nFeatures': [12660, 12660, 16165, 16165, 16165, 11261, 13707, 2633, 16842], 'nb': [0.77825464949928469, 0.77825464949928469, 0.5, 0.5, 0.74892703862660948, 0.77753934191702434, 0.77682403433476388, 0.77825464949928469, 0.77467811158798272]}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

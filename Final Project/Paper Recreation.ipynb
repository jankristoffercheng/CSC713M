{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thumbs up? Sentiment Classification using Machine Learning Techniques\n",
    "<ul>\n",
    "    <li>by Bo Pang, Lillian Lee and Shivakumar Vaithyanathan</li>\n",
    "    <li>recreated by <b>Jan Kristoffer Cheng</b> and <b>Johansson Tan</b></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook tries to recreate the results from the paper of Pang, Lee and Vaithyanathan regarding sentiment classification. They used movie reviews from IMDb as their corpus and classified the reviews as having either a positive or negative sentiment. In order to be able to do binary classification, they built different models using different features and machine learning techniques. The machine learning techniques that they used were Naive Bayes, Maximum Entropy, and SVM, but this project will only use Naive Bayes and SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Reading the corpus</h3>\n",
    "\n",
    "The corpus is readily available online. It contains different versions with each version having cleaner data. The results from the paper used version 0.9, but we used version 1.0. \n",
    "\n",
    "The zip file when extracted is split into two folders neg and pos, with each having 700 text files falling into the corresponding category. The class <i>FileReader</i> reads all the files given a path. This is also where the punctuations are separated from the word to easily distinguish them later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative: 700\n",
      "Positive: 700\n",
      "Total: 1400\n"
     ]
    }
   ],
   "source": [
    "from file_reader import FileReader\n",
    "\n",
    "negPath = 'mix20_rand700_tokens_cleaned/tokens/neg/'\n",
    "posPath = 'mix20_rand700_tokens_cleaned/tokens/pos/'\n",
    "\n",
    "fileReader = FileReader()\n",
    "\n",
    "negatives = fileReader.getTexts(negPath)\n",
    "positives = fileReader.getTexts(posPath)\n",
    "allTexts = negatives + positives\n",
    "\n",
    "print('Negative:', len(negatives))\n",
    "print('Positive:', len(positives))\n",
    "print('Total:', len(allTexts))\n",
    "\n",
    "N = len(negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Appending tags for negations</h3>\n",
    "\n",
    "The class <i>TextNegator</i> appends <b>--n</b> to words between a negation and punctuation. The output from this function is used for unigrams. As an example, consider the sentence: <b>I don't like the movie. I didn't enjoy at all.</b> The punctuations from this example will be split from the word as this is already done in <i>FileReader</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I don't like--n the--n movie--n . I didn't enjoy--n at--n all--n .\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from features import TextNegator\n",
    "\n",
    "texts = [\"I don't like the movie . I didn't enjoy at all .\"]\n",
    "textNegator = TextNegator()\n",
    "textNegator.getNegated(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of negations and punctuations used for <i>TextNegator</i> are inside the features file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both texts in negative and positive will be processed by <i>TextNegator</i> for latter use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "negatedNegatives = textNegator.getNegated(negatives)\n",
    "negatedPositives = textNegator.getNegated(positives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<h3>Preparing libraries</h3>\n",
    "\n",
    "Before anything else, different libraries such as numpy and sklearn should be imported. They will be utilized in building the models later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from k_fold import KFoldBatcher\n",
    "\n",
    "nFold = 3\n",
    "nPerFold = int(N/nFold)\n",
    "print(nPerFold)\n",
    "\n",
    "kfold = KFold(nFold)\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the paper, we used 3-fold cross validation with each fold having 233 texts from each category. The class <i>KFoldBatcher</i> splits the dimensions and classes into batches for cross validation. The variable <i>results</i> will hold the different outputs such as features used, average number of features, and average accuracies of both Naive Bayes and SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Unigrams frequency</h3>\n",
    "\n",
    "```python\n",
    "class UnigramFeature:\n",
    "    def __init__(self):\n",
    "        self.unigrams = []\n",
    "    def process(self, negatedTexts):\n",
    "    def get(self, negatedTexts, type='pres'):\n",
    "```\n",
    "\n",
    "<i>UnigramFeature.process(negatedTexts)</i> saves all the unigrams that appeared at least 4 times in the training data and stores it in <i>self.unigrams</i>.\n",
    "\n",
    "<i>UnigramFeature.get(negatedTexts, type='freq')</i> returns the unigrams' frequencies of the negated texts as a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features: 12660\n",
      "Naive Bayes Accuracy: 0.778254649499\n",
      "SVM Accuracy: 0.782546494993\n"
     ]
    }
   ],
   "source": [
    "from features import UnigramFeature\n",
    "\n",
    "nbAccuracy = 0\n",
    "svmAccuracy = 0\n",
    "nFeatures = 0\n",
    "i = 0\n",
    "for trainIndex, testIndex in kfold.split(negatives):\n",
    "    unigramFeature = UnigramFeature()\n",
    "    unigramFeature.process([negatedNegatives[index] for index in trainIndex] + [negatedPositives[index] for index in trainIndex])\n",
    "    nFeatures += len(unigramFeature.unigrams)\n",
    "    \n",
    "    featuresNegative = unigramFeature.get(negatedNegatives, type='freq')\n",
    "    featuresPositive = unigramFeature.get(negatedPositives, type='freq')\n",
    "    \n",
    "    kfoldBatcher = KFoldBatcher(nFold, featuresNegative, featuresPositive)\n",
    "    trainX = kfoldBatcher.getTrainX(i)\n",
    "    trainY = kfoldBatcher.getTrainY(i)\n",
    "    \n",
    "    testX = kfoldBatcher.getTestX(i)\n",
    "    testY = kfoldBatcher.getTestY(i)\n",
    "    \n",
    "    nb = BernoulliNB()\n",
    "    nb.fit(trainX, trainY)\n",
    "    nbAccuracy += accuracy_score(nb.predict(testX), testY)\n",
    "\n",
    "    svm = LinearSVC()\n",
    "    svm.fit(trainX, trainY)\n",
    "    svmAccuracy += accuracy_score(svm.predict(testX), testY)\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "nbAccuracy /= nFold\n",
    "svmAccuracy /= nFold\n",
    "nFeatures = int(nFeatures/nFold)\n",
    "result = {\n",
    "    'features': 'unigrams', \n",
    "    'nFeatures': nFeatures, \n",
    "    'freqPres': 'freq', \n",
    "    'nb': nbAccuracy, \n",
    "    'svm': svmAccuracy\n",
    "}\n",
    "results.append(result)\n",
    "\n",
    "print('Number of Features:', nFeatures)\n",
    "print('Naive Bayes Accuracy:', nbAccuracy)\n",
    "print('SVM Accuracy:', svmAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Unigrams presence</h3>\n",
    "\n",
    "In contrast before, the models here utilize the presence of the unigrams. Thus, each feature will either have a value of 0 or 1.\n",
    "\n",
    "<i>UnigramFeature.get(negatedTexts, type='freq')</i> returns the unigrams' presence of the negated texts as a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features: 12660\n",
      "Naive Bayes Accuracy: 0.778254649499\n",
      "SVM Accuracy: 0.792560801144\n"
     ]
    }
   ],
   "source": [
    "unigramFeaturesNegative = []\n",
    "unigramFeaturesPositive = []\n",
    "\n",
    "nbAccuracy = 0\n",
    "svmAccuracy = 0\n",
    "nFeatures = 0\n",
    "i = 0\n",
    "for trainIndex, testIndex in kfold.split(negatives):\n",
    "    unigramFeature = UnigramFeature()\n",
    "    unigramFeature.process([negatedNegatives[index] for index in trainIndex] + [negatedPositives[index] for index in trainIndex])\n",
    "    nFeatures += len(unigramFeature.unigrams)\n",
    "    \n",
    "    featuresNegative = unigramFeature.get(negatedNegatives, type='pres')\n",
    "    featuresPositive = unigramFeature.get(negatedPositives, type='pres')\n",
    "    \n",
    "    unigramFeaturesNegative.append(featuresNegative)\n",
    "    unigramFeaturesPositive.append(featuresPositive)\n",
    "\n",
    "    kfoldBatcher = KFoldBatcher(nFold, featuresNegative, featuresPositive)\n",
    "    \n",
    "    trainX = kfoldBatcher.getTrainX(i)\n",
    "    trainY = kfoldBatcher.getTrainY(i)\n",
    "    \n",
    "    testX = kfoldBatcher.getTestX(i)\n",
    "    testY = kfoldBatcher.getTestY(i)\n",
    "    \n",
    "    nb = BernoulliNB()\n",
    "    nb.fit(trainX, trainY)\n",
    "    nbAccuracy += accuracy_score(nb.predict(testX), testY)\n",
    "\n",
    "    svm = LinearSVC()\n",
    "    svm.fit(trainX, trainY)\n",
    "    svmAccuracy += accuracy_score(svm.predict(testX), testY)\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "nbAccuracy /= nFold\n",
    "svmAccuracy /= nFold\n",
    "nFeatures = int(nFeatures/nFold)\n",
    "result = {\n",
    "    'features': 'unigrams', \n",
    "    'nFeatures': nFeatures, \n",
    "    'freqPres': 'pres', \n",
    "    'nb': nbAccuracy, \n",
    "    'svm': svmAccuracy\n",
    "}\n",
    "results.append(result)\n",
    "\n",
    "print('Number of Features:', nFeatures)\n",
    "print('Naive Bayes Accuracy:', nbAccuracy)\n",
    "print('SVM Accuracy:', svmAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Bigrams</h3>\n",
    "\n",
    "```python\n",
    "class BigramFeature:\n",
    "    def __init__(self):\n",
    "        self.bigrams = []\n",
    "    def process(self, texts):\n",
    "    def get(self, texts):\n",
    "```\n",
    "\n",
    "<i>BigramFeature.process(texts)</i> saves the top 16165 bigrams that appeared at least 7 times in the training data and stores it in <i>self.bigrams</i>.\n",
    "\n",
    "<i>BigramFeature.get(texts)</i> returns the presence of the bigrams of the texts as a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features: 16165\n",
      "Naive Bayes Accuracy: 0.745350500715\n",
      "SVM Accuracy: 0.752503576538\n"
     ]
    }
   ],
   "source": [
    "from features import BigramFeature\n",
    "\n",
    "bigramFeaturesNegative = []\n",
    "bigramFeaturesPositive = []\n",
    "\n",
    "nbAccuracy = 0\n",
    "svmAccuracy = 0\n",
    "nFeatures = 0\n",
    "i = 0\n",
    "for trainIndex, testIndex in kfold.split(negatives):\n",
    "    bigramFeature = BigramFeature()\n",
    "    bigramFeature.process([negatives[index] for index in trainIndex] + [positives[index] for index in trainIndex])\n",
    "    nFeatures += len(bigramFeature.bigrams)\n",
    "    \n",
    "    featuresNegative = bigramFeature.get(negatives)\n",
    "    featuresPositive = bigramFeature.get(positives)\n",
    "    \n",
    "    bigramFeaturesNegative.append(featuresNegative)\n",
    "    bigramFeaturesPositive.append(featuresPositive)\n",
    "\n",
    "    kfoldBatcher = KFoldBatcher(nFold, featuresNegative, featuresPositive)\n",
    "    \n",
    "    trainX = kfoldBatcher.getTrainX(i)\n",
    "    trainY = kfoldBatcher.getTrainY(i)\n",
    "    \n",
    "    testX = kfoldBatcher.getTestX(i)\n",
    "    testY = kfoldBatcher.getTestY(i)\n",
    "    \n",
    "    nb = BernoulliNB()\n",
    "    nb.fit(trainX, trainY)\n",
    "    nbAccuracy += accuracy_score(nb.predict(testX), testY)\n",
    "\n",
    "    svm = LinearSVC()\n",
    "    svm.fit(trainX, trainY)\n",
    "    svmAccuracy += accuracy_score(svm.predict(testX), testY)\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "nbAccuracy /= nFold\n",
    "svmAccuracy /= nFold\n",
    "nFeatures = int(nFeatures/nFold)\n",
    "result = {\n",
    "    'features': 'bigrams', \n",
    "    'nFeatures': nFeatures, \n",
    "    'freqPres': 'pres', \n",
    "    'nb': nbAccuracy, \n",
    "    'svm': svmAccuracy\n",
    "}\n",
    "results.append(result)\n",
    "\n",
    "print('Number of Features:', nFeatures)\n",
    "print('Naive Bayes Accuracy:', nbAccuracy)\n",
    "print('SVM Accuracy:', svmAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Unigrams and Bigrams</h3>\n",
    "\n",
    "The models here utilize both unigrams and bigrams, which is essentially concatenating both features before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features: 28825\n",
      "Naive Bayes Accuracy: 0.776824034335\n",
      "SVM Accuracy: 0.808297567954\n"
     ]
    }
   ],
   "source": [
    "nbAccuracy = 0\n",
    "svmAccuracy = 0\n",
    "nFeatures = 0\n",
    "for i in range(nFold):\n",
    "    featuresNegative = np.concatenate((unigramFeaturesNegative[i], bigramFeaturesNegative[i]), axis=1)\n",
    "    featuresPositive = np.concatenate((unigramFeaturesPositive[i], bigramFeaturesPositive[i]), axis=1)\n",
    "    nFeatures += unigramFeaturesNegative[i].shape[1] + bigramFeaturesNegative[i].shape[1]\n",
    "\n",
    "    kfoldBatcher = KFoldBatcher(nFold, featuresNegative, featuresPositive)\n",
    "    \n",
    "    trainX = kfoldBatcher.getTrainX(i)\n",
    "    trainY = kfoldBatcher.getTrainY(i)\n",
    "    \n",
    "    testX = kfoldBatcher.getTestX(i)\n",
    "    testY = kfoldBatcher.getTestY(i)\n",
    "    \n",
    "    nb = BernoulliNB()\n",
    "    nb.fit(trainX, trainY)\n",
    "    nbAccuracy += accuracy_score(nb.predict(testX), testY)\n",
    "\n",
    "    svm = LinearSVC()\n",
    "    svm.fit(trainX, trainY)\n",
    "    svmAccuracy += accuracy_score(svm.predict(testX), testY)\n",
    "    \n",
    "nbAccuracy /= nFold\n",
    "svmAccuracy /= nFold\n",
    "nFeatures = int(nFeatures/nFold)\n",
    "result = {\n",
    "    'features': 'unigrams+bigrams', \n",
    "    'nFeatures': nFeatures, \n",
    "    'freqPres': 'pres', \n",
    "    'nb': nbAccuracy, \n",
    "    'svm': svmAccuracy\n",
    "}\n",
    "results.append(result)\n",
    "\n",
    "print('Number of Features:', nFeatures)\n",
    "print('Naive Bayes Accuracy:', nbAccuracy)\n",
    "print('SVM Accuracy:', svmAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Unigrams + POS</h3>\n",
    "\n",
    "The class <i>POSTagger</i> returns the part-of-speech sequence using the nltk library given a string. As an example, consider the sentence: <b>The movie was really great! I didn't expect that plot twist!</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['DT',\n",
       "  'NN',\n",
       "  'VBD',\n",
       "  'RB',\n",
       "  'JJ',\n",
       "  '.',\n",
       "  'PRP',\n",
       "  'VBP',\n",
       "  'VB',\n",
       "  'IN',\n",
       "  'NN',\n",
       "  'NN',\n",
       "  '.']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from features import POSTagger\n",
    "\n",
    "texts = [\"The movie was really great ! I didn't expect that plot twist !\"]\n",
    "posTagger = POSTagger()\n",
    "posTagger.getPOS(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the POS sequences of the corpus for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posNegatives = posTagger.getPOS(negatives)\n",
    "posPositives = posTagger.getPOS(positives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class UnigramPOSFeature:\n",
    "    def __init__(self):\n",
    "        self.unigrams = []\n",
    "    def process(self, negatedTexts, posOfTexts):\n",
    "    def get(self, negatedTexts, posOfTexts):\n",
    "```\n",
    "\n",
    "<i>UnigramPOSFeature.process(negatedTexts, posOfTexts)</i> saves the unique unigrams and POS combination that appeared at least 4 times in the training data and stores it in <i>self.unigrams</i>.\n",
    "\n",
    "<i>UnigramPOSFeature.get(negatedTexts, posOfTexts)</i> returns the presence of the unigrams of the texts as a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features: 13707\n",
      "Naive Bayes Accuracy: 0.776824034335\n",
      "SVM Accuracy: 0.79113018598\n"
     ]
    }
   ],
   "source": [
    "from features import UnigramPOSFeature\n",
    "\n",
    "nbAccuracy = 0\n",
    "svmAccuracy = 0\n",
    "nFeatures = 0\n",
    "i = 0\n",
    "for trainIndex, testIndex in kfold.split(negatives):\n",
    "    unigramPOSFeature = UnigramPOSFeature()\n",
    "    negatedTextsTrain = [negatedNegatives[index] for index in trainIndex] + [negatedPositives[index] for index in trainIndex]\n",
    "    posTextsTrain = [posNegatives[index] for index in trainIndex] + [posPositives[index] for index in trainIndex]\n",
    "    unigramPOSFeature.process(negatedTextsTrain, posTextsTrain)\n",
    "    nFeatures += len(unigramPOSFeature.unigrams)\n",
    "    \n",
    "    featuresNegative = unigramPOSFeature.get(negatedNegatives, posNegatives)\n",
    "    featuresPositive = unigramPOSFeature.get(negatedPositives, posPositives)\n",
    "\n",
    "    kfoldBatcher = KFoldBatcher(nFold, featuresNegative, featuresPositive)\n",
    "    \n",
    "    trainX = kfoldBatcher.getTrainX(i)\n",
    "    trainY = kfoldBatcher.getTrainY(i)\n",
    "    \n",
    "    testX = kfoldBatcher.getTestX(i)\n",
    "    testY = kfoldBatcher.getTestY(i)\n",
    "    \n",
    "    nb = BernoulliNB()\n",
    "    nb.fit(trainX, trainY)\n",
    "    nbAccuracy += accuracy_score(nb.predict(testX), testY)\n",
    "\n",
    "    svm = LinearSVC()\n",
    "    svm.fit(trainX, trainY)\n",
    "    svmAccuracy += accuracy_score(svm.predict(testX), testY)\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "nbAccuracy /= nFold\n",
    "svmAccuracy /= nFold\n",
    "nFeatures = int(nFeatures/nFold)\n",
    "result = {\n",
    "    'features': 'unigrams+POS', \n",
    "    'nFeatures': nFeatures, \n",
    "    'freqPres': 'pres', \n",
    "    'nb': nbAccuracy, \n",
    "    'svm': svmAccuracy\n",
    "}\n",
    "results.append(result)\n",
    "\n",
    "print('Number of Features:', nFeatures)\n",
    "print('Naive Bayes Accuracy:', nbAccuracy)\n",
    "print('SVM Accuracy:', svmAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Adjectives</h3>\n",
    "\n",
    "```python\n",
    "class AdjectiveFeature:\n",
    "    def __init__(self):\n",
    "        self.adjectives = []\n",
    "    def process(self, texts, posOfTexts):\n",
    "    def get(self, texts):\n",
    "```\n",
    "\n",
    "<i>AdjectiveFeature.process(texts, posOfTexts)</i> saves the adjectives that appeared in the training data and stores it in <i>self.adjectives</i>.\n",
    "\n",
    "<i>AdjectiveFeature.get(texts)</i> returns the presence of the adjectives of the texts as a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from features import AdjectiveFeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features: 11261\n",
      "Naive Bayes Accuracy: 0.777539341917\n",
      "SVM Accuracy: 0.76251788269\n"
     ]
    }
   ],
   "source": [
    "nbAccuracy = 0\n",
    "svmAccuracy = 0\n",
    "nFeatures = 0\n",
    "i = 0\n",
    "for trainIndex, testIndex in kfold.split(negatives):\n",
    "    adjFeature = AdjectiveFeature()\n",
    "    textsTrain = [negatives[index] for index in trainIndex] + [positives[index] for index in trainIndex]\n",
    "    posTextsTrain = [posNegatives[index] for index in trainIndex] + [posPositives[index] for index in trainIndex]\n",
    "    adjFeature.process(textsTrain, posTextsTrain)\n",
    "    nFeatures += len(adjFeature.adjectives)\n",
    "    \n",
    "    featuresNegative = adjFeature.get(negatives)\n",
    "    featuresPositive = adjFeature.get(positives)\n",
    "\n",
    "    kfoldBatcher = KFoldBatcher(nFold, featuresNegative, featuresPositive)\n",
    "    \n",
    "    trainX = kfoldBatcher.getTrainX(i)\n",
    "    trainY = kfoldBatcher.getTrainY(i)\n",
    "    \n",
    "    testX = kfoldBatcher.getTestX(i)\n",
    "    testY = kfoldBatcher.getTestY(i)\n",
    "    \n",
    "    nb = BernoulliNB()\n",
    "    nb.fit(trainX, trainY)\n",
    "    nbAccuracy += accuracy_score(nb.predict(testX), testY)\n",
    "\n",
    "    svm = LinearSVC()\n",
    "    svm.fit(trainX, trainY)\n",
    "    svmAccuracy += accuracy_score(svm.predict(testX), testY)\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "nbAccuracy /= nFold\n",
    "svmAccuracy /= nFold\n",
    "nFeatures = int(nFeatures/nFold)\n",
    "result = {\n",
    "    'features': 'adjectives', \n",
    "    'nFeatures': nFeatures, \n",
    "    'freqPres': 'pres', \n",
    "    'nb': nbAccuracy, \n",
    "    'svm': svmAccuracy\n",
    "}\n",
    "results.append(result)\n",
    "\n",
    "print('Number of Features:', nFeatures)\n",
    "print('Naive Bayes Accuracy:', nbAccuracy)\n",
    "print('SVM Accuracy:', svmAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Top 2633 unigrams</h3>\n",
    "\n",
    "This model utilizes the top 2633 unigrams as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features: 2633\n",
      "Naive Bayes Accuracy: 0.778254649499\n",
      "SVM Accuracy: 0.792560801144\n"
     ]
    }
   ],
   "source": [
    "nbAccuracy = 0\n",
    "svmAccuracy = 0\n",
    "nFeatures = 0\n",
    "for i in range(nFold):\n",
    "    featuresNegative = unigramFeaturesNegative[i][:2633]\n",
    "    featuresPositive = unigramFeaturesPositive[i][:2633]\n",
    "\n",
    "    kfoldBatcher = KFoldBatcher(nFold, featuresNegative, featuresPositive)\n",
    "    \n",
    "    trainX = kfoldBatcher.getTrainX(i)\n",
    "    trainY = kfoldBatcher.getTrainY(i)\n",
    "    \n",
    "    testX = kfoldBatcher.getTestX(i)\n",
    "    testY = kfoldBatcher.getTestY(i)\n",
    "    \n",
    "    nb = BernoulliNB()\n",
    "    nb.fit(trainX, trainY)\n",
    "    nbAccuracy += accuracy_score(nb.predict(testX), testY)\n",
    "\n",
    "    svm = LinearSVC()\n",
    "    svm.fit(trainX, trainY)\n",
    "    svmAccuracy += accuracy_score(svm.predict(testX), testY)\n",
    "    \n",
    "nbAccuracy /= nFold\n",
    "svmAccuracy /= nFold\n",
    "nFeatures = 2633\n",
    "result = {\n",
    "    'features': 'top 2633 unigrams', \n",
    "    'nFeatures': nFeatures, \n",
    "    'freqPres': 'pres', \n",
    "    'nb': nbAccuracy, \n",
    "    'svm': svmAccuracy\n",
    "}\n",
    "results.append(result)\n",
    "\n",
    "print('Number of Features:', nFeatures)\n",
    "print('Naive Bayes Accuracy:', nbAccuracy)\n",
    "print('SVM Accuracy:', svmAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Unigrams + position</h3>\n",
    "\n",
    "The class <i>PositionTagger</i> returns the position sequence given a string, considering the first quarter, middle and last quarter. As an example, consider the sentence: <b>The movie was really great! I didn't expect that plot twist!</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from features import PositionTagger\n",
    "\n",
    "texts = [\"The movie was really great ! I didn't expect that plot twist !\"]\n",
    "positionTagger = PositionTagger()\n",
    "positionTagger.getPositions(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the example, when the sentence is splitted, it results to 12 words. Therefore, each quarter should have 3 words. The position tag of the first 3 words is 0, the position tag of the middle part is 1, and the position tag of the last quarter is 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the position sequences of the corpus for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positionNegatives = positionTagger.getPositions(negatedNegatives)\n",
    "positionPositives = positionTagger.getPositions(negatedPositives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class UnigramPositionFeature:\n",
    "    def __init__(self):\n",
    "        self.unigrams = []\n",
    "    def process(self, negatedTexts, positionsOfTexts):\n",
    "    def get(self, negatedTexts, positionsOfTexts):\n",
    "```\n",
    "\n",
    "<i>UnigramPositionFeature.process(negatedTexts, positionsOfTexts)</i> saves the combination of unigram and position that appeared at least 4 times in the training data and stores it in <i>self.unigrams</i>.\n",
    "\n",
    "<i>UnigramPositionFeature.get(negatedTexts, positionsOfTexts)</i> returns the presence of the unigrams of the texts as a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from features import UnigramPositionFeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features: 16842\n",
      "Naive Bayes Accuracy: 0.774678111588\n",
      "SVM Accuracy: 0.786123032904\n"
     ]
    }
   ],
   "source": [
    "nbAccuracy = 0\n",
    "svmAccuracy = 0\n",
    "nFeatures = 0\n",
    "i = 0\n",
    "for trainIndex, testIndex in kfold.split(negatives):\n",
    "    unigramPositionFeature = UnigramPositionFeature()\n",
    "    negatedTextsTrain = [negatedNegatives[index] for index in trainIndex] + [negatedPositives[index] for index in trainIndex]\n",
    "    positionTextsTrain = [positionNegatives[index] for index in trainIndex] + [positionPositives[index] for index in trainIndex]\n",
    "    unigramPositionFeature.process(negatedTextsTrain, positionTextsTrain)\n",
    "    nFeatures += len(unigramPositionFeature.unigrams)\n",
    "    \n",
    "    featuresNegative = unigramPositionFeature.get(negatedNegatives, positionNegatives)\n",
    "    featuresPositive = unigramPositionFeature.get(negatedPositives, positionPositives)\n",
    "\n",
    "    kfoldBatcher = KFoldBatcher(nFold, featuresNegative, featuresPositive)\n",
    "    \n",
    "    trainX = kfoldBatcher.getTrainX(i)\n",
    "    trainY = kfoldBatcher.getTrainY(i)\n",
    "    \n",
    "    testX = kfoldBatcher.getTestX(i)\n",
    "    testY = kfoldBatcher.getTestY(i)\n",
    "    \n",
    "    nb = BernoulliNB()\n",
    "    nb.fit(trainX, trainY)\n",
    "    nbAccuracy += accuracy_score(nb.predict(testX), testY)\n",
    "\n",
    "    svm = LinearSVC()\n",
    "    svm.fit(trainX, trainY)\n",
    "    svmAccuracy += accuracy_score(svm.predict(testX), testY)\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "nbAccuracy /= nFold\n",
    "svmAccuracy /= nFold\n",
    "nFeatures = int(nFeatures/nFold)\n",
    "result = {\n",
    "    'features': 'unigrams+position', \n",
    "    'nFeatures': nFeatures, \n",
    "    'freqPres': 'pres', \n",
    "    'nb': nbAccuracy, \n",
    "    'svm': svmAccuracy\n",
    "}\n",
    "results.append(result)\n",
    "\n",
    "print('Number of Features:', nFeatures)\n",
    "print('Naive Bayes Accuracy:', nbAccuracy)\n",
    "print('SVM Accuracy:', svmAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Results</h3>\n",
    "\n",
    "This block of code is for displaying the results in table format for better presentation. The results from this recreation and the original paper will be compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<center><b>Table 1 Average three-fold cross validation accuracies</b>\n",
       "    <table>\n",
       "        <tr>\n",
       "            <th></th>\n",
       "            <th>Features</th>\n",
       "            <th># of features</th>\n",
       "            <th>frequency or presence?</th>\n",
       "            <th>NB</th>\n",
       "            <th>SVM</th>\n",
       "        </tr>\n",
       "<td>(1)</td><td>unigrams</td><td>12660</td><td>freq</td><td>77.8</td><td>78.3</td></tr><td>(2)</td><td>unigrams</td><td>12660</td><td>pres</td><td>77.8</td><td>79.3</td></tr><td>(3)</td><td>bigrams</td><td>16165</td><td>pres</td><td>74.5</td><td>75.3</td></tr><td>(4)</td><td>unigrams+bigrams</td><td>28825</td><td>pres</td><td>77.7</td><td>80.8</td></tr><td>(5)</td><td>unigrams+POS</td><td>13707</td><td>pres</td><td>77.7</td><td>79.1</td></tr><td>(6)</td><td>adjectives</td><td>11261</td><td>pres</td><td>77.8</td><td>76.3</td></tr><td>(7)</td><td>top 2633 unigrams</td><td>2633</td><td>pres</td><td>77.8</td><td>79.3</td></tr><td>(8)</td><td>unigrams+position</td><td>16842</td><td>pres</td><td>77.5</td><td>78.6</td></tr></table></center>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "html = '<center><b>Table 1 Average three-fold cross validation accuracies</b>'\n",
    "\n",
    "html += '''\n",
    "    <table>\n",
    "        <tr>\n",
    "            <th></th>\n",
    "            <th>Features</th>\n",
    "            <th># of features</th>\n",
    "            <th>frequency or presence?</th>\n",
    "            <th>NB</th>\n",
    "            <th>SVM</th>\n",
    "        </tr>\n",
    "'''\n",
    "\n",
    "for i in range(len(results)):\n",
    "    result = results[i]\n",
    "    html += '<td>(' + str(i+1) + ')</td>' + '<td>' + result['features'] + '</td>' + '<td>' + str(result['nFeatures']) + '</td>' + '<td>' + result['freqPres'] + '</td>' + '<td>' + str(round(result['nb']*100, 1)) + '</td>' + '<td>' + str(round(result['svm']*100,1)) + '</td>' + '</tr>'\n",
    "\n",
    "html += '</table></center>'\n",
    "    \n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<center><b>Table 2 Average three-fold cross validation accuracies from the paper</b></center>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th></th>\n",
    "        <th>Features</th>\n",
    "        <th># of features</th>\n",
    "        <th>frequency or presence?</th>\n",
    "        <th>NB</th>\n",
    "        <th>SVM</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>(1)</td>\n",
    "        <td>unigrams</td>\n",
    "        <td>16165</td>\n",
    "        <td>freq</td>\n",
    "        <td>78.7</td>\n",
    "        <td>72.8</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>(2)</td>\n",
    "        <td>unigrams</td>\n",
    "        <td>16165</td>\n",
    "        <td>pres</td>\n",
    "        <td>81.0</td>\n",
    "        <td>82.9</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>(3)</td>\n",
    "        <td>bigrams</td>\n",
    "        <td>16165</td>\n",
    "        <td>pres</td>\n",
    "        <td>77.3</td>\n",
    "        <td>77.1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>(4)</td>\n",
    "        <td>unigrams+bigrams</td>\n",
    "        <td>32330</td>\n",
    "        <td>pres</td>\n",
    "        <td>80.6</td>\n",
    "        <td>82.7</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>(5)</td>\n",
    "        <td>unigrams+POS</td>\n",
    "        <td>16695</td>\n",
    "        <td>pres</td>\n",
    "        <td>81.5</td>\n",
    "        <td>81.9</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>(6)</td>\n",
    "        <td>adjectives</td>\n",
    "        <td>2633</td>\n",
    "        <td>pres</td>\n",
    "        <td>77.0</td>\n",
    "        <td>75.1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>(7)</td>\n",
    "        <td>top 2633 unigrams</td>\n",
    "        <td>2633</td>\n",
    "        <td>pres</td>\n",
    "        <td>80.3</td>\n",
    "        <td>81.4</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>(8)</td>\n",
    "        <td>unigrams+position</td>\n",
    "        <td>22430</td>\n",
    "        <td>pres</td>\n",
    "        <td>81.0</td>\n",
    "        <td>81.6</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above show the accuracies in percent. Table 1 shows the results of the recreation, while Table 2 shows the results from the paper. The results recreated are different from the results of the original paper, but they are close to each other. The main reason behind this might be due to having different preprocessing methods such as tokenization including punctuation detection, text negation, and POS tagging.\n",
    "\n",
    "For tokenization, the sentences were only splitted by spaces and the punctuations were only detected if they are at the end of a word. For text negation, the paper does not explicitly say what negation words were used so we listed our own negation words (see features.py -> NEGATIONS variable). As for POS tagging, different libraries were used. We used the NLTK library, while they used Oliver Mason's QTag program.\n",
    "\n",
    "The researchers of the study intend to prove that Naive Bayes performs well in sentiment classification. It is almost always the case that Support Vector Machines(SVM) outperforms Naive Bayes(NB) in different classification problems, but not when it comes to sentiment classification. The results show that the performace of SVM and NB are at par with each other.\n",
    "\n",
    "Further study can be done on sentiment classification to increase the performance of the models. Some recommendations are having cleaner data and combining the different features and see if it "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
